{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Symbolic Links and Split Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def delete_symbolic_links(target_dir):\n",
    "    \"\"\"\n",
    "    Delete all symbolic links in the specified directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_dir):\n",
    "        print(f\"Directory {target_dir} does not exist.\")\n",
    "        return\n",
    "\n",
    "    for filename in os.listdir(target_dir):\n",
    "        file_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        # Check if the file is a symbolic link\n",
    "        if os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "            # print(f\"Deleted symbolic link: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Skipped (not a link): {file_path}\")\n",
    "\n",
    "def split_data_and_create_links(source_dirs, train_dir, val_dir, test_dir, positive, split_ratio=(0.6, 0.2, 0.2)):\n",
    "    # Function to create directories and copy links\n",
    "    # This function seems to be having a problem, the symbolic links are being created but they are somehow broken and don't work\n",
    "    def create_dir_and_symlink_files(file_list, source_dir, target_dir):\n",
    "        source_dir = os.path.abspath(source_dir)\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "\n",
    "        for filename in file_list:\n",
    "            source_file = os.path.join(source_dir, filename)\n",
    "            target_file = os.path.join(target_dir, filename)\n",
    "\n",
    "            if os.path.isfile(source_file) and not os.path.exists(target_file):\n",
    "                os.symlink(source_file, target_file)\n",
    "\n",
    "    # Process each source directory\n",
    "    for source_dir in source_dirs:\n",
    "        if not os.path.exists(source_dir):\n",
    "            print(f\"Source directory {source_dir} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # Get all filenames in the source directory\n",
    "        filenames = os.listdir(source_dir)\n",
    "        random.shuffle(filenames)  # Shuffle to randomize distribution\n",
    "\n",
    "        # Calculate split indices\n",
    "        total_files = len(filenames)\n",
    "        train_end = int(total_files * split_ratio[0])\n",
    "        val_end = train_end + int(total_files * split_ratio[1])\n",
    "\n",
    "        # Split filenames into training, validation, and test sets\n",
    "        train_filenames = filenames[:train_end]\n",
    "        val_filenames = filenames[train_end:val_end]\n",
    "        test_filenames = filenames[val_end:]\n",
    "\n",
    "        # Create directories and copy the files\n",
    "        pos_or_neg = 'positive' if positive else 'negative'\n",
    "        create_dir_and_symlink_files(train_filenames, source_dir, os.path.join(train_dir, pos_or_neg))\n",
    "        create_dir_and_symlink_files(val_filenames, source_dir, os.path.join(val_dir, pos_or_neg))\n",
    "        create_dir_and_symlink_files(test_filenames, source_dir, os.path.join(test_dir, pos_or_neg))\n",
    "\n",
    "# Delete old symbolic links:\n",
    "for path in ['../data/train/positive', '../data/train/negative', '../data/val/positive', '../data/val/negative', '../data/test/positive', '../data/test/negative']:\n",
    "    delete_symbolic_links(path)\n",
    "\n",
    "# Source directories\n",
    "pos_image_paths = ['../all_brain_images/manual_label']\n",
    "neg_image_paths = ['../all_non_brain/manual_label']\n",
    "\n",
    "# Split data and create new directories with symbolic links\n",
    "split_data_and_create_links(pos_image_paths, '../data/train', '../data/val', '../data/test', positive=True)\n",
    "split_data_and_create_links(neg_image_paths, '../data/train', '../data/val', '../data/test', positive=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\josep\\miniconda3\\envs\\brain_classifier_env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 450 images belonging to 2 classes.\n",
      "Found 149 images belonging to 2 classes.\n",
      "Found 152 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Data augmentation for the training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  # VGG16 preprocessing\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Only rescaling for the validation and test data\n",
    "val_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Paths to train, validation, and test directories\n",
    "train_dir = os.path.normpath('../data/train')\n",
    "val_dir = os.path.normpath('../data/val')\n",
    "test_dir = os.path.normpath('../data/test')\n",
    "\n",
    "\n",
    "# Data generators for train, validation, and test sets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False  # Usually, you don't shuffle the test data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\josep\\miniconda3\\envs\\brain_classifier_env\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\josep\\miniconda3\\envs\\brain_classifier_env\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\josep\\miniconda3\\envs\\brain_classifier_env\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\josep\\miniconda3\\envs\\brain_classifier_env\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\josep\\miniconda3\\envs\\brain_classifier_env\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "14/14 [==============================] - 67s 5s/step - loss: 12.2269 - accuracy: 0.7751 - val_loss: 1.2593 - val_accuracy: 0.9219\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 64s 5s/step - loss: 4.1987 - accuracy: 0.8906 - val_loss: 2.0094 - val_accuracy: 0.9141\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 70s 5s/step - loss: 1.9182 - accuracy: 0.9139 - val_loss: 1.1760 - val_accuracy: 0.9141\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 57s 4s/step - loss: 1.2958 - accuracy: 0.9330 - val_loss: 0.4874 - val_accuracy: 0.9297\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 58s 4s/step - loss: 0.8674 - accuracy: 0.9450 - val_loss: 0.6125 - val_accuracy: 0.9375\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 57s 4s/step - loss: 0.3081 - accuracy: 0.9498 - val_loss: 0.3521 - val_accuracy: 0.9688\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.3501 - accuracy: 0.9522 - val_loss: 0.4057 - val_accuracy: 0.9375\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 60s 4s/step - loss: 0.4653 - accuracy: 0.9487 - val_loss: 0.1328 - val_accuracy: 0.9688\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 56s 4s/step - loss: 0.3066 - accuracy: 0.9522 - val_loss: 0.4155 - val_accuracy: 0.9375\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 57s 4s/step - loss: 0.1163 - accuracy: 0.9689 - val_loss: 0.1238 - val_accuracy: 0.9531\n",
      "5/5 [==============================] - 15s 3s/step - loss: 0.7524 - accuracy: 0.9474\n",
      "Test accuracy: 0.9473684430122375, Test loss: 0.7523719072341919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\miniconda3\\envs\\brain_classifier_env\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.applications import VGG16\n",
    "\n",
    "# Load VGG16 model, pre-trained on ImageNet data\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,            # Base model (VGG16)\n",
    "    Flatten(),             # Flatten the output\n",
    "    Dense(256, activation='relu'),  # A dense layer\n",
    "    Dropout(0.5),          # Dropout for regularization\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 10  # Set the number of epochs\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=num_epochs\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_accuracy}, Test loss: {test_loss}\")\n",
    "\n",
    "model.save('../models/brain_img_classifier_2.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_classifier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
